\fancyhead[LH]{寒武纪所处行业格局分析}
\rhead{\includegraphics[scale=0.25]{fig/particle_future.png}}  %在此处插入logo.pdf图片 图片靠左

\section{寒武纪所处行业格局分析}
\subsection{寒武纪所处产业链}
人工智能芯片产业链主要分为上游的材料与设备，中游的产品制造，下游
的应用市场；上游的材料与设备主要指半导体材料和半导体设备，半导体材料
包括单晶硅、单晶锗、砷化镓、晶体管等材料，半导体设备包括光刻机、等离子
刻蚀机等设备；中游的产品制造包括芯片设计和芯片制造，芯片设计的流程主
要是通过 EDA 进行系统设计、RTL 设计、物理设计等过程，芯片制造包括晶圆
加工、晶圆测试、晶片切割、芯片封装等过程；下游的应用市场主要有云计算、
自动驾驶、智能手机、无人机、智能音箱、智能安防等，整体架构如图1-2所示。\par

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/industry_chain.png}
    \caption{AI芯片行业产业链}
    \label{industry_chain}
  \end{figure}
  寒武纪专注于智能芯片的设计和销售,而将晶圆制造、封装测试等其余环节委托给晶圆制造
  企业、封装测试企业及其他加工厂商代工完成，因此寒武纪位于AI芯片产业链的中游，整个AI芯片产业链分布如图\ref{industry_chain}，
  整个产业链的关键节点处的知名企业分布如图\ref{enterprise_chain}。
  \begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/enterprise_chain.png}
    \caption{AI芯片产业链中的知名企业}
    \label{enterprise_chain}
  \end{figure}

\subsubsection{产业链重大变化}
\begin{itemize}
    \item 下游应用：ChatGPT热浪来袭，LLM热度居高不下，各大厂商都在布局LLM，打造以LLM为核心的生态，
    例如微软已经将ChatGPT嵌入bing搜索，AI聊天也将登录谷歌搜索，下游端对于算力的要求会越来越高，专注于AI领域的芯片将持续高热，
    地平线 CEO 余凯表示 ASIC 有望成为未来方向。
    \item 基于Chiplet的先进封装越来越多投入使用，相关产业受到越来越多的关注。
\end{itemize}
\subsection{寒武纪所处行业竞争格局}

\subsubsection{国内外技术对比}
以下列举了部分国际巨头厂商的产品，对其进行简要分析，并与
寒武纪公司的产品进行了对比。

\begin{itemize}
    \item NVIDIA H100(2022年)：\textbf{主要用于实时深度学习推理}，项先进技术可将推理速度提高 30 倍，并提供超低的延迟。
    Transformer 引擎可结合使用 FP8 和 FP16 精度，减少内存占用并提高性能，同时仍能保持大型语言模型的准确性。
    在产品参数方面，H100 SXM的性能可达$3958TOPS^*(INT8)$推理算力，GPU显存达到$80GB$，GPU显存带宽$3.35TB/s$，最大热功率高达$700W$;
    H100 PCIe的性能可达$3026TOPS^*(INT8)$,GPU显存达到$80GB$，GPU显存带宽$2TB/s$，最大热功率高达$300-350W$。
    \item NVIDIA A100(2020年):A100 采用 NVIDIA Ampere 架构，\textbf{可以用于深度学习训练与推理}，是 NVIDIA 数据中心平台的引擎，A100 提供 40GB 和 80GB 显存两种版本，
    80GB显存版本将GPU显存增加了一倍，并提供超快速的显存带宽（每秒超过 2 万亿字节 [TB/s]），可处理超大型模型和数据集。在产品参数方面，
    A100\quad 80GB\quad PCIe的性能可达$624TOPS^*(INT8)$推理算力,GPU显存达到$80GB$，GPU显存带宽$1935GB/s$，最大热功率达$300W$；
    A100\quad 80GB\quad SXM的性能可达$1248TOPS^*(INT8)$推理算力,GPU显存达到$80GB$，GPU显存带宽$2039GB/s$，最大热功率达$400W$。
    \item Intel Habana Gaudi2(2022):主要用于深度学习模型训练，虽然英特尔并未公布Habana Gaudi2的细节参数，但是根据英特尔展示的性能对比数据来看，
    Habana Gaudi2在RestNet50 Training Throughput和BERT Tralning Throughput等视频及自然语言处理的模型测试中，性能都达到了NVIDIA A100的2倍左右。
    \item Google TPUv4(2021年):，GoogleTPU v4的实力不俗，在使用ImageNet数据集的图像分类训练测试（准确度至少$75.90\%$），
    256 个TPU v4在1.82分钟内完成了这一任务，这几乎与768个Nvidia A100图形卡、192个AMD Epyc 7742内核（1.06分钟）、512个华为AI优化的Ascend910芯片以及
    128个Intel Xeon Platinum 8168内核（1.56分钟）组合在一起的速度一样快。
    \item 华为海思 Ascend 910:HUAWEI Ascend 910基于自研华为达芬奇架构3D Cube技术，实现业界最佳AI性能与能效，架构灵活伸缩，支持云边端全栈全场景应用。
    算力方面，昇腾910完全达到设计规格，拥有$640TOPS^*(INT8)$的推理算力，功耗310W。
    \item 寒武纪 思元370加速板卡MLU370-S4(2022年)：\textbf{面向高密度云端推理}，MLU370-S4加速卡采用思元370芯片，TSMC 7nm制程，寒武纪新一代人工智能芯片架构MLUarch03加持，
    支持PCIe Gen4，板载24GB低功耗高带宽LPDDR5内存，板卡功耗仅为75W，相较于同尺寸GPU，可提供3倍的解码能力和1.5倍的编码能力。MLU370-S4加速卡的能效出色，体积小巧，
    可在服务器中实现高密度部署。在产品参数方面，其峰值性能为$192TOPS^*(INT8)$推理算力，内存容量$24GB$，内存带宽$307.2GB/s$，最大热功耗$75W$。
    \item 寒武纪 思元370加速板卡MLU370-X4(2022年)：MLU370-X4加速卡采用思元370芯片，为单槽位150w全尺寸加速卡，可提供高达$256TOPS^*(INT8)$推理算力，
    和$24TFLOPS(FP32)$训练算力，同时提供丰富的FP16、BF16等多种训练精度，配合全新基础系统软件平台，可充分满足推训一体AI任务需求。
    \item 寒武纪 思元290系列MLU290-M5智能加速卡(2021年)：搭载寒武纪首颗训练芯片思元290，采用台积电7nm先进制程工艺，采用MLUv02扩展架构。MLU290-M5智能加速卡采用开放加速模块OAM设计，
    具备64个MLU Core，1.23TB/s内存带宽以及全新MLU-Link芯片间互联技术，全面支持AI训练、推理或混合型人工智能计算加速任务，性能方面可达$TOPS512^*(INT8)$。
\end{itemize}

目前寒武纪虽然是国内第一梯队的芯片设计商，但在于国际芯片巨头厂商对比时仍存在较大差距。从上述陈列的产品信息可以发现，寒武纪思元370是2022年推出的产品，而英伟达A100是2020年的产品，
\textbf{从AI推理算力层面看仍然落后于NVIDIA}，而寒武纪在2021年推出的思元290，定位与A100类似，在各种精度上都存在一定差距。但华为昇腾910在AI推理性能方面确实与NVIDIA相差无几。
图\ref{H100_score}与图\ref{cambricon_score}是基于上述分析做出的评分对比，
选取的产品分别是NVIDIA H100 PCIe与寒武纪 思元370加速板卡MLU370-S4，评分带有较强的主观性与数据局限性，
仅供参考。\par
英伟达研发为底、生态为径，英伟达重视
研发投入，提高产品性能，同时打造了CUDA软件平台，使得利用 GPU 来训练神经网络等高算力模型的
难度大大降低，将 GPU 的应用从 3D 游戏和图像处理拓展到科学计算、大数据处理、
机器学习等领域，这一生态系统的建立让很多开发者依赖于CUDA。与英伟达相比，
寒武纪需要自己对原生深度学习框架进行修改以支持思元芯片，
而英伟达有谷歌原厂支持。硬件方面，从一些表观的性能参数对比来看，寒武纪训练芯片思元290和英伟达A100、昇腾910相比性能还有追赶的空间。
软件方面，寒武纪是自己对原生的Tensorflow和Pytorch深度学习框架去针对自己的思元芯片去做修改而非像华为一样自研深度学习框架去进行优化，
也不像英伟达一样因为芯片市占率高，有Pytorch/Tensorflow原厂去做GPU算子的优化和设备的支持。
另外寒武纪相比英伟达的算子库丰富程度以及软件工具链的完善程度还有一定差距，需要时间去追赶。
\begin{figure}[htb]
  \centering
\begin{minipage}{0.49\linewidth}
  \centering
  \includegraphics[width=0.9\linewidth]{fig/NVIDIA_score.png}
  \caption{NVIDIA H100 PCIe评分}\label{H100_score}
\end{minipage}
\begin{minipage}{0.47\linewidth}
  \centering
  \includegraphics[width=0.9\linewidth]{fig/cambricon.png}
  \caption{思元370加速板卡评分}\label{cambricon_score}
\end{minipage}
\end{figure}
\subsubsection{从AI芯片部署场景看竞争格局}
\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\linewidth]{fig/competition.png}
  \caption{AI芯片根据部署场景划分的竞争格局}
  \label{competition}
\end{figure}
根据部署场景，AI 芯片可用于端、边、云三种场景，具体如图\ref{competition}而言：
\begin{enumerate}
  \item 终端 AI 芯片追求以低功耗完成推理任务，以实际落地场景需求为导向。终端芯片主要追求低功耗、高性能并以推理任务为主、成本敏感、硬件产品形态众多。
  \item 边缘 AI 芯片位于终端和云端之间的传输网络，一般不具备终端的数据采集功能，主要完成数据的汇聚、处理分析和通信传输。边缘AI芯片以推理任务为主，外设丰富（如同时输入多路摄像头的数据），算力一般比终端AI芯片强，满足安全性和及时性需求，并降低回传的网络带宽占用。
  \item 云端 AI 芯片以高算力/完成训练任务为目标，包括 CPU/GPU/FPGA/ASIC 等多种类型。
\end{enumerate}

\textbf{在云端领域}:云端训练用的几乎全部是英伟达GPU，公有云厂商中仅谷歌云一家除了提供以英伟达GPU为主的云计算加速服务之外，
还推出了基于自研AI芯片TPU的深度学习训练服务。云端推理目前出现了基于GPU、FPGA、ASIC三种不同芯片云计算服务，但是市场份额仍然以英伟达GPU为主。\par
\textbf{在边缘计算领域}:
边缘芯片较云端竞争格局较分散，NVIDIA凭借Jetson系列芯片占优势地位。NVIDA 2021年面向性能需求高但尺寸/功耗受限的边缘计算设备推出了Jetson Xavier NX，
能够在较低功耗下提供较高算力，同时可以在CUDA-X AI软件架构上运行。
国内寒武纪于2019年推出了边缘智能芯片思元220及相应的加速卡，支持边缘计算场景下的多种AI应用。
除此以外，海外厂商高通、英特尔、谷歌及国内厂商华为、地平线等也推出针对边缘计算的AI芯片，竞争激烈。\par
\textbf{在终端领域}：终端主要是集成芯片，其中AI模块主要采用的是ASIC芯片，因此竞争格局与ASIC芯片的竞争格局息息相关。ASIC针对性强，细分领域较多，
行业竞争呈现多点开花的趋势，国外许多芯片巨头布局ASIC领域，国内发展势头也正盛。
\subsubsection{市占率分析}
根据寒武纪2023年7月5日在投资者关系平台上的答复可知，寒武纪目前深耕国内用户，而NVIDIA凭借软件生态优势占据了$90\%$以上的份额，包括寒武纪在内的其它
主流芯片和加速卡方案提供商所占市场份额暂时较小。\par
在国内寒武纪的销售表现可圈可点。寒武纪智能处理器 IP 产品已集成于超过1亿台智能手机及其他智能终端设备中，思元系列产品也已应用于浪潮、
联想等多家服务器厂商的产品中。此外，思元 270 芯片、思元 290 芯片还分别获得第六届世界互
联网大会、世界人工智能大会颁布的奖项。思元 220 自发布以来，累计销量突破百万片。\textbf{需要注意的是}，寒武纪许多产品用户单一，产品销售过于依赖少数用户。\par

国产AI 算力芯片正起星星之火，投融资热度高企。根据电子发烧友统计，2022 年多
家 AI 芯片公司获得大额融资，其中摩尔线程达 15 亿元、天数智芯超 10 亿元、
沐曦达 10 亿元。在国产 AI 算力芯片中，除寒武纪外，
海光信息 DCU 产品深算一号部分参数对标英伟达 A100，但是产品具体参数在官网并没有提供。国内非上市公司中，
天数智芯的训练侧产品 BI 达到 295TOPS INT8 算力；沐曦的推理侧产品
MXN100 已于 2022 年 8 月回片点亮。互联网巨头亦强势入局 AI 芯片，腾讯领
投的燧原科技推出推理侧产品云燧 i20，INT8 算力达 256TOPS；百度孵化的昆
仑芯推出训推一体 AI 芯片 R200，INT8 算力达 256TOPS；背靠阿里的平头哥亦
早在 2019 年就推出推理侧 AI 芯片含光 800。
